import optuna
import os
import sys
import logging
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, VecNormalize

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from utils.data_loader import DataLoader
from strategies.trading_core import TradingCore
from models.rl_env import CryptoEnv

# [ì„¤ì •] ë¡œê·¸ ë„ê¸°
config.SYSTEM['SUPPRESS_WARNINGS'] = True
optuna.logging.set_verbosity(optuna.logging.WARNING)

# ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ (ìºì‹±)
LOADER = DataLoader()
DF_MAIN = LOADER.get_ml_data(config.MAIN_SYMBOL)

def objective(trial):
    # 1. ìµœì í™”í•  ë³´ìƒ íŒŒë¼ë¯¸í„° (Reward Engineering)
    reward_params = {
        'profit_scale': trial.suggest_int('profit_scale', 100, 500, step=50),
        'teacher_bonus': trial.suggest_float('teacher_bonus', 0.0, 0.2),
        'teacher_penalty': trial.suggest_float('teacher_penalty', 0.0, 0.3),
        'mdd_penalty_factor': trial.suggest_float('mdd_penalty', 0.5, 2.0),
        
        # [ì¶”ê°€] ì „ê³ ì  ê°±ì‹  ë³´ë„ˆìŠ¤ (ê¸°ì¡´ 0.5 í•˜ë“œì½”ë”© -> ìµœì í™”)
        'new_high_bonus': trial.suggest_float('new_high_bonus', 0.1, 1.0)
    }
    
    # 2. RL í•˜ì´í¼íŒŒë¼ë¯¸í„°
    lr = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)
    
    # 3. í™˜ê²½ ìƒì„± í•¨ìˆ˜
    def make_env():
        env = CryptoEnv(DF_MAIN, TradingCore(), precision_df=None, debug=False)
        env.reward_params = reward_params # íŒŒë¼ë¯¸í„° ì£¼ì…
        return env

    # 4. ì•½ì‹ í•™ìŠµ (Short Training)
    # CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° n_envs ì¡°ì ˆ (ì˜ˆ: 6~8)
    n_envs = config.SYSTEM['NUM_WORKERS'] 
    train_steps = 300000 
    
    env = SubprocVecEnv([make_env for _ in range(n_envs)])
    env = VecMonitor(env)
    env = VecNormalize(env, norm_obs=True, norm_reward=True, gamma=0.99)
    
    # RL Device ì„¤ì • (config ì°¸ì¡°)
    device = config.SYSTEM['MAIN_RL_DEVICE']
    model = PPO("MlpPolicy", env, learning_rate=lr, verbose=0, device=device, n_steps=1024, batch_size=1024)
    
    try:
        model.learn(total_timesteps=train_steps)
        # í‰ê°€: ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ
        mean_reward = np.mean([ep['r'] for ep in env.ep_info_buffer])
        return mean_reward
        
    except Exception as e:
        return -99999
    finally:
        env.close()

if __name__ == "__main__":
    print("ğŸš€ Starting RL Reward Optimization (Short Training Mode)...")
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=50) # 50ë²ˆ ì‹œë„
    
    print("ğŸ† Best Reward Params:", study.best_params)